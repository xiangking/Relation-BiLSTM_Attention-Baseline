{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "# Authors: Xiang Wang(wx15857152367@163.com)\n",
    "# Date: 2019/04/22 12:10:00\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 设置需要使用的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf8 -*-\n",
    "\"\"\"\n",
    "该模型主要用于将文本转化为id形式，同时提供分词和训练词向量的静态函数\n",
    "\"\"\"\n",
    "\n",
    "import jieba\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from zhon.hanzi import punctuation\n",
    "from gensim.models import Word2Vec\n",
    "from functools import lru_cache\n",
    "\n",
    "class Vocab(object):\n",
    "    \n",
    "    def __init__(self, filename=None,  initial_tokens=None):\n",
    "        self.id2token = {}\n",
    "        self.token2id = {}\n",
    "\n",
    "        self.embed_dim = None\n",
    "        self.embeddings = None\n",
    "\n",
    "        self.pad_token = '<blank>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.e1s = '<e1>'\n",
    "        self.e1e = '</e1>'\n",
    "        self.e2s = '<e2>'\n",
    "        self.e2e = '</e2>'\n",
    "        \n",
    "        self.initial_tokens = initial_tokens.copy() if initial_tokens is not None else []\n",
    "        self.initial_tokens.insert(0, self.e1s)\n",
    "        self.initial_tokens.insert(0, self.e1e)\n",
    "        self.initial_tokens.insert(0, self.e2s)\n",
    "        self.initial_tokens.insert(0, self.e2e)\n",
    "        self.initial_tokens.insert(0, self.unk_token)\n",
    "        self.initial_tokens.insert(0, self.pad_token)\n",
    "        \n",
    "        for token in self.initial_tokens:\n",
    "            self.add(token)\n",
    "            \n",
    "            \n",
    "    def add(self, token) -> int:\n",
    "        \"\"\"\n",
    "        Adds the token to vocab\n",
    "        Args:\n",
    "            token: a string\n",
    "        \"\"\"\n",
    "        if token in self.token2id:\n",
    "            idx = self.token2id[token]\n",
    "        else:\n",
    "            idx = len(self.id2token)\n",
    "            self.id2token[idx] = token\n",
    "            self.token2id[token] = idx\n",
    "\n",
    "        return idx \n",
    "    \n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of vocabulary\n",
    "        Returns:\n",
    "            an integer indicating the size\n",
    "        \"\"\"\n",
    "        return len(self.id2token)\n",
    "    \n",
    "    def get_id(self, token) -> int:\n",
    "        \"\"\"\n",
    "        Gets the id of a token, returns the id of unk token if token is not in vocab\n",
    "        Args:\n",
    "            key: a string indicating the word\n",
    "        Returns:\n",
    "            an integer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.token2id[token]\n",
    "        except KeyError:\n",
    "            return self.token2id[self.unk_token]\n",
    "        \n",
    "    def get_token(self, idx) -> str:\n",
    "        \"\"\"\n",
    "        Gets the token corresponding to idx, returns unk token if idx is not in vocab\n",
    "        Args:\n",
    "            idx: an integer\n",
    "        returns:\n",
    "            a token string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.id2token[idx]\n",
    "        except KeyError:\n",
    "            return self.unk_token\n",
    "        \n",
    "    def convert_to_ids(self, tokens) -> list:\n",
    "        \"\"\"\n",
    "        Convert a list of tokens to ids, use unk_token if the token is not in vocab.\n",
    "        Args:\n",
    "            tokens: a list of token\n",
    "        Returns:\n",
    "            a list of ids\n",
    "        \"\"\"\n",
    "        vec = [self.get_id(term) for term in tokens]\n",
    "        return vec\n",
    "    \n",
    "    def recover_from_ids(self, ids, stop_id=None) -> list:\n",
    "        \"\"\"\n",
    "        Convert a list of ids to tokens, stop converting if the stop_id is encountered\n",
    "        Args:\n",
    "            ids: a list of ids to convert\n",
    "            stop_id: the stop id, default is None\n",
    "        Returns:\n",
    "            a list of tokens\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens += [self.get_token(i)]\n",
    "            if stop_id is not None and i == stop_id:\n",
    "                break\n",
    "        return tokens\n",
    "    \n",
    "    def recover_id2token(self) -> dict:\n",
    "        \"\"\"\n",
    "        Rebuild the id2token\n",
    "        Returns:\n",
    "            a dict about converting id to token\n",
    "        \"\"\"\n",
    "        id2token_temp = {}\n",
    "        for token_iter, idx_iter in self.token2id:\n",
    "            id2token_temp[idx_iter] = token_iter\n",
    "            \n",
    "        return id2token_temp\n",
    "                    \n",
    "    \n",
    "    def load_pretrained_embeddings(self, trained_embeddings):\n",
    "        \"\"\"\n",
    "        Loads the pretrained embeddings\n",
    "        Args:\n",
    "            trained_embeddings: the pretrained embeddings\n",
    "        \"\"\"        \n",
    "        if self.embed_dim is None:\n",
    "            self.embed_dim = len(trained_embeddings[0])\n",
    "\n",
    "        # load embeddings\n",
    "        self.embeddings = np.zeros([self.size, self.embed_dim])\n",
    "        for idx, trained_vec in enumerate(trained_embeddings):\n",
    "            self.embeddings[idx+6] = trained_vec \n",
    "            \n",
    "\n",
    "    def randomly_init_embeddings(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Randomly initializes the embeddings for each token\n",
    "        Args:\n",
    "            embed_dim: the size of the embedding for each token\n",
    "        \"\"\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embeddings = np.random.rand(self.size, embed_dim)\n",
    "        for token in [self.pad_token, self.unk_token]:\n",
    "            self.embeddings[self.get_id(token)] = np.zeros([self.embed_dim])\n",
    "            \n",
    "    \n",
    "    def save(self, mode='pkl', base_dir = '.'):\n",
    "        \"\"\"\n",
    "        Save the dict and embedding\n",
    "        Args:\n",
    "            mode: the way to save\n",
    "            base_dir: the root path of the save file\n",
    "        \"\"\"\n",
    "        print(\"保存字典和词向量...........\\n\")\n",
    "        model_dir = f'{base_dir}/model'\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "            \n",
    "        if mode == 'pkl':\n",
    "            with open(f'{model_dir}/word_idx_dict.pkl', 'wb') as f:\n",
    "                pickle.dump(self.token2id , f)\n",
    "            \n",
    "            with open(f'{model_dir}/word_vectors_arr.pkl', 'wb') as f:\n",
    "                pickle.dump(self.embeddings, f)\n",
    "        \n",
    "        if mode == 'txt':\n",
    "            with open(f'{model_dir}/word_idx_dict.pkl', 'wb') as f:\n",
    "                for token_iter, id_iter in self.token2id.items():\n",
    "                    f.write(token_iter + ' ' + id_iter + '\\n')\n",
    "                    \n",
    "            with open(f'{model_dir}/word_idx_dict.pkl', 'wb') as f:\n",
    "                for embedding_vec_iter in self.embeddings:\n",
    "                    f.write(embedding_vec_iter + '\\n')\n",
    "                \n",
    "        print(\"字典和词向量已保存到该目录下！\\n\")\n",
    "        \n",
    "        \n",
    "    def load(self, mode='pkl', base_dir = '.'):\n",
    "        \"\"\"\n",
    "        Loads the dict and embedding \n",
    "        Args:\n",
    "            mode: the way to save\n",
    "            base_dir: the root path of the load file\n",
    "        \"\"\"\n",
    "        print(\"加载字典和词向量...........\\n\")\n",
    "                \n",
    "        model_dir = f'{base_dir}/model'\n",
    "            \n",
    "        if mode == 'pkl':\n",
    "            with open(f'{model_dir}/word_idx_dict.pkl', 'rb') as f:\n",
    "                self.token2id = pickle.load(f)\n",
    "            \n",
    "            self.id2token = self.recover_id2token()\n",
    "            \n",
    "                \n",
    "            with open(f'{model_dir}/word_vectors_arr.pkl', 'rb') as f:\n",
    "                self.embeddings = pickle.load(f)\n",
    "        # TO DO        \n",
    "        if mode == 'txt':\n",
    "            pass\n",
    "                \n",
    "        print(\"字典和词向量已保存到该目录下！\\n\")\n",
    "\n",
    "             \n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text, filter_stop_word = None, lower=False) -> list:\n",
    "        \"\"\"\n",
    "        The function to tokenize\n",
    "        Args:\n",
    "            filter_stop_word: the stop words list\n",
    "            lower: lower or not\n",
    "        \"\"\"\n",
    "        text = re.sub(r'[%s]+' % punctuation, '', text) \n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "        tokens = jieba.lcut(text)\n",
    "        if filter_stop_word:\n",
    "            stop_word_set = set(filter_stop_word)\n",
    "            tokens = filter(lambda w: w not in stop_word_set, tokens)\n",
    "        return list(tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_word2vec(sentences, win_size=1, vec_size=50, base_dir = '.'):\n",
    "        \"\"\"\n",
    "        Train a word2vec model\n",
    "        Args:\n",
    "            sentences: the train data\n",
    "            base_dir: the root path of the load file\n",
    "        \"\"\"\n",
    "        time_s = time.time()\n",
    "        print ('begin to train model...')\n",
    "        w2v_model = Word2Vec(sentences=sentences, \n",
    "                             size=vec_size, \n",
    "                             window=win_size, \n",
    "                             min_count=1,\n",
    "                             workers=20, \n",
    "                             sg=1, \n",
    "                             iter=25, \n",
    "                             hs=0)\n",
    "        print(\"train model success\\n\")\n",
    "\n",
    "        word2vec_dir = f'{base_dir}/word2vec'\n",
    "        if not os.path.exists(word2vec_dir):\n",
    "            os.makedirs(word2vec_dir)\n",
    "        \n",
    "        w2v_model.save(f'{word2vec_dir}/word2vec.model')\n",
    "        print ('save model success, model_path=%s, time=%.4f sec.' \n",
    "                % (f'{word2vec_dir}/word2vec.model', time.time() - time_s))\n",
    "        \n",
    "        return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import nltk\n",
    "\n",
    "\n",
    "class RCDataset(object):\n",
    "    \"\"\"\n",
    "    This module implements the APIs for loading and using  dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_file=[], dev_file=[], test_file=[], pad_id=0):\n",
    "\n",
    "        self.logger = logging.getLogger(\"RC\")\n",
    "\n",
    "        self.pattern_symbol = re.compile(r'[\\(\\)\\[\\]\\{\\},:;!~@^_$¥`<>]')\n",
    "\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.train_contents = []\n",
    "        self.dev_contents = []\n",
    "        self.test_contents = []\n",
    "\n",
    "        self.train_labels = []\n",
    "        self.dev_labels = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        if train_file:\n",
    "            self.train_contents, self.train_labels = self._load_dataset(train_file)\n",
    "            self.logger.info('Train set size: {} titles.'.format(len(self.train_labels)))\n",
    "\n",
    "        if dev_file:\n",
    "            self.dev_contents, self.dev_labels = self._load_dataset(dev_file)\n",
    "            self.logger.info('Dev set size: {} titles.'.format(len(self.dev_labels)))\n",
    "\n",
    "        if test_file:\n",
    "            self.test_contents, self.test_labels = self._load_dataset(test_file)\n",
    "            self.logger.info('Test set size: {} titles.'.format(len(self.test_labels)))\n",
    "        \n",
    "        self.max_content_len = len(max(self.all_contents,key=lambda x:len(x))) + 4\n",
    "\n",
    "        self.raw_test_contents = self.test_contents.copy()\n",
    "\n",
    "        self.categories = sorted(list(set(self.train_labels + self.dev_labels + self.test_labels)))\n",
    "        self.cat2id = dict(zip(self.categories, range(len(self.categories))))\n",
    "        self.id2cat = dict(zip(range(len(self.categories)), self.categories))\n",
    "\n",
    "        self.num_class = len(self.cat2id)\n",
    "\n",
    "    def _load_dataset(self, data_path):\n",
    "        \"\"\"\n",
    "        Loads the dataset\n",
    "        Args:\n",
    "            data_path: the data file to load\n",
    "        \"\"\"\n",
    "        with open(data_path, mode='r', encoding='utf-8',errors='ignore') as fin:\n",
    "            lines = fin.readlines()\n",
    "            contents, relations = [], []\n",
    "\n",
    "            for i in range(0, len(lines), 4):\n",
    "                relation = lines[i + 1].strip()\n",
    "                content = lines[i].strip().split('\\t')[1][1:-2].lower()\n",
    "                content = self._process_data(content)\n",
    "                contents.append(content)\n",
    "                relations.append(str(relation))\n",
    "\n",
    "        return contents, relations\n",
    "\n",
    "\n",
    "    def _process_data(self, content):\n",
    "        \n",
    "        tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "        content = content.replace(\"'\", \" '\")\n",
    "        content = content.replace(\",\", \" ,\")\n",
    "        content = content.replace(\".\", \" .\")\n",
    "        content = content.replace('<e1>', '<e1> ')\n",
    "        content = content.replace('</e1>', ' </e1>')\n",
    "        content = content.replace('<e2>', '<e2> ')\n",
    "        content = content.replace('</e2>', ' </e2>')\n",
    "        content = re.sub('\\d+\\.\\d+', 'NUMERICAL', content)\n",
    "        content = re.sub('[a-zA-Z\\.]+\\.com', 'URL', content)\n",
    "        content = re.sub('[a-zA-Z\\.]+\\.org', 'URL', content)\n",
    "        content = ' '.join(tokenizer.tokenize(content))\n",
    "\n",
    "        content = content.split(' ')        \n",
    "\n",
    "        return content\n",
    "\n",
    "    def convert_to_ids(self, vocab):\n",
    "        \"\"\"\n",
    "        Convert the tokens to ids\n",
    "        Args:\n",
    "            vocab: the convert vocab\n",
    "        \"\"\"\n",
    "\n",
    "        if self.train_contents:\n",
    "            self.train_contents = [[vocab.convert_to_ids(contents)]\n",
    "                                   for contents in self.train_contents]\n",
    "\n",
    "        if self.dev_contents:\n",
    "            self.dev_contents = [[vocab.convert_to_ids(contents)]\n",
    "                                 for contents in self.dev_contents]\n",
    "\n",
    "        if self.test_contents:\n",
    "            self.test_contents = [[vocab.convert_to_ids(contents)]\n",
    "                                  for contents in self.test_contents]\n",
    "\n",
    "    @property\n",
    "    def all_contents(self) -> list:\n",
    "        \"\"\"\n",
    "        Get all data\n",
    "        Args:\n",
    "            the list of all data\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.train_contents + self.dev_contents + self.test_contents\n",
    "\n",
    "    @property\n",
    "    def all_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get the all labels\n",
    "        Returns:\n",
    "            the list of all labels\n",
    "        \"\"\"\n",
    "        return self.train_labels + self.dev_labels + self.test_labels\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of all data\n",
    "        Returns:\n",
    "            an integer indicating the size\n",
    "        \"\"\"\n",
    "        return len(self.train_labels + self.dev_labels + self.test_labels)\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def train_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of train data\n",
    "        Returns:\n",
    "            the size of train data\n",
    "        \"\"\"\n",
    "        return len(self.train_labels)\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def dev_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of dev data\n",
    "        Returns:\n",
    "            the size of dev data\n",
    "        \"\"\"\n",
    "        return len(self.dev_labels)\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def test_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of  test data\n",
    "        Returns:\n",
    "            the size of test data\n",
    "        \"\"\"\n",
    "        return len(self.test_labels)\n",
    "\n",
    "    def _dynamic_padding(self, batch_data):\n",
    "        \"\"\"\n",
    "        Dynamically pads the batch_data with pad_id\n",
    "        \"\"\"\n",
    "        pad_content_len = self.max_content_len\n",
    "        batch_data['contents'] = [(ids + [self.pad_id] *(pad_content_len - len(ids)))[:pad_content_len] \n",
    "                                  for ids in batch_data['contents']]\n",
    "        return batch_data\n",
    "\n",
    "\n",
    "    def _one_mini_batch(self, data) -> dict:\n",
    "        \"\"\"\n",
    "        Get one mini batch\n",
    "        Args:\n",
    "            data: all data\n",
    "        Returns:\n",
    "            one batch of data\n",
    "        \"\"\"\n",
    "        batch_data = {\n",
    "            'contents': [list(contents)[0] for contents in data[0]],\n",
    "            'labels': data[1],\n",
    "            'contents_length': []\n",
    "        }\n",
    "        \n",
    "        labelId_list = []\n",
    "        for sent_idx, content in enumerate(batch_data['contents']):\n",
    "            \n",
    "            batch_data['contents_length'].append(len(content))\n",
    "            lid = self.cat2id[batch_data['labels'][sent_idx]]\n",
    "            labelId_list.append(lid)\n",
    "\n",
    "        batch_data['labels'] = tf.keras.utils.to_categorical(labelId_list, num_classes=len(self.categories))\n",
    "\n",
    "        batch_data = self._dynamic_padding(batch_data)\n",
    "\n",
    "        return batch_data\n",
    "\n",
    "    def gen_mini_batches(self, set_name='train', batch_size=256):\n",
    "        \"\"\"\n",
    "        Generate  batches\n",
    "        Args:\n",
    "            set_name: the mode\n",
    "            batch_size: the size of one batch\n",
    "        \"\"\"\n",
    "        if set_name == 'train':\n",
    "            x = self.train_contents\n",
    "            y = self.train_labels\n",
    "            shuffle = True\n",
    "        elif set_name == 'dev':\n",
    "            x = self.dev_contents\n",
    "            y = self.dev_labels\n",
    "            shuffle = False\n",
    "        elif set_name == 'test':\n",
    "            x = self.test_contents\n",
    "            y = self.test_labels\n",
    "            shuffle = False\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'No data set named as {}'.format(setName))\n",
    "\n",
    "        data_len = len(x)\n",
    "        num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(np.arange(data_len))\n",
    "            x_shuffle = np.array(x)[indices]\n",
    "            y_shuffle = np.array(y)[indices]\n",
    "        else:\n",
    "            x_shuffle = x\n",
    "            y_shuffle = y\n",
    "\n",
    "        for i in range(num_batch):\n",
    "            start_id = i * batch_size\n",
    "            end_id = min((i + 1) * batch_size, data_len)\n",
    "            yield self._one_mini_batch([x_shuffle[start_id:end_id],\n",
    "                                        y_shuffle[start_id:end_id]])\n",
    "\n",
    "    def get_category(self, idx):\n",
    "        \"\"\"\n",
    "        Get the category corresponding to idx, returns None if idx is not in vocab\n",
    "        Args:\n",
    "            idx: an integer\n",
    "        returns:\n",
    "            a token string or None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.id2cat[idx]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def save(self, id_categories_dict_path, is_id2categories=True):\n",
    "        \"\"\"\n",
    "        Save the needed data\n",
    "        Args:\n",
    "            id_categories_dict_path: save path\n",
    "            is_id2categories: whether to save the dict of id2categories\n",
    "        \"\"\"\n",
    "        with open(id_categories_dict_path, 'wb') as f:\n",
    "            pickle.dump(self.id2cat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 实际过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "dataSet = RCDataset(train_file='./raw_data/train.txt', \n",
    "                    dev_file='./raw_data/test.txt',\n",
    "                    test_file='./raw_data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 词向量和词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 加载本地已有的词向量和词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vocab_words_list = []\n",
    "\n",
    "with open('./raw_data/pretrained_w2v/vocab.txt', mode='r', encoding='utf-8',errors='ignore') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for word in lines:\n",
    "        load_vocab_words_list.append(word.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "word_2x = numpy.load('./raw_data/pretrained_w2v/w2v_50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(initial_tokens=load_vocab_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.load_pretrained_embeddings(word_2x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 生成词向量和词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vecModel = Vocab.train_word2vec(dataSet.train_contents, base_dir='../data')\n",
    "# vocab = Vocab(initial_tokens=word2vecModel.wv.index2word)\n",
    "# vocab.load_pretrained_embeddings(word2vecModel.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 将文本转化成ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.convert_to_ids(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、模型的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(object):\n",
    "    \"\"\"\n",
    "    Implements the Attention layer\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "    def compute_attention(self, inputs_encodes, return_alphas=False):\n",
    "        \"\"\"\n",
    "        Compute the self attention of BiLSTM Output\n",
    "        Args:\n",
    "            inputs_encodes: output of BiLSTM\n",
    "            return_alphas: whether to return alphas\n",
    "        returns:\n",
    "            last: the result about using self-attention\n",
    "            alphas(OR): the attention\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('attention'):\n",
    "            sequence_length = inputs_encodes.shape[1].value  # the length of sequences processed in the antecedent RNN layer\n",
    "\n",
    "            # Attention mechanism\n",
    "            attention_w = tf.Variable(tf.random_normal([self.hidden_size,  self.attention_size], stddev=0.1))\n",
    "            attention_b = tf.Variable(tf.random_normal([self.attention_size], stddev=0.1))\n",
    "            attention_u = tf.Variable(tf.random_normal([self.attention_size], stddev=0.1))\n",
    "\n",
    "            v = tf.tanh(tf.matmul(tf.reshape(inputs_encodes, [-1, self.hidden_size ]), attention_w) \n",
    "                        + tf.reshape(attention_b, [1, -1]))\n",
    "            vu = tf.matmul(v, tf.reshape(attention_u, [-1, 1]))\n",
    "            exps = tf.reshape(tf.exp(vu), [-1, sequence_length])\n",
    "            alphas = exps / tf.reshape(tf.reduce_sum(exps, 1), [-1, 1])\n",
    "\n",
    "            last = tf.reduce_sum(inputs_encodes * tf.reshape(alphas, [-1, sequence_length, 1]), 1)\n",
    "\n",
    "            if not return_alphas:\n",
    "                return last\n",
    "            else:\n",
    "                return last, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(object):\n",
    "    \"\"\"\n",
    "    Implements the Attention layer\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "    def compute_attention(self, inputs_encodes, return_alphas=False):\n",
    "        \"\"\"\n",
    "        Compute the self attention of BiLSTM Output\n",
    "        Args:\n",
    "            inputs_encodes: output of BiLSTM\n",
    "            return_alphas: whether to return alphas\n",
    "        returns:\n",
    "            last: the result about using self-attention\n",
    "            alphas(OR): the attention\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('attention'):\n",
    "            sequence_length = inputs_encodes.shape[1].value  # the length of sequences processed in the antecedent RNN layer\n",
    "\n",
    "            # Attention mechanism\n",
    "            attention_w = tf.get_variable('attention_omega', \n",
    "                                          [self.hidden_size, 1], \n",
    "                                          initializer=tf.keras.initializers.glorot_normal(), \n",
    "                                          trainable=True)\n",
    "            tanh_lstm_feature = tf.tanh(inputs_encodes)\n",
    "            tanh_lstm_feature = tf.reshape(tanh_lstm_feature, [-1, self.hidden_size])#bz*n, num_filter\n",
    "            alph = tf.nn.softmax(tf.reshape(tf.matmul(tanh_lstm_feature,attention_w),[-1, sequence_length]))\n",
    "            alph = tf.reshape(alph,[-1, 1, sequence_length])\n",
    "            last = tf.nn.tanh(tf.reshape(tf.matmul(alph, inputs_encodes),[-1, self.hidden_size]))#bz,num_filter\n",
    "\n",
    "            if not return_alphas:\n",
    "                return last\n",
    "            else:\n",
    "                return last, alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tc\n",
    "\n",
    "\n",
    "\n",
    "def rnn(rnn_type, inputs, length, hidden_size, layer_num=1, dropout_keep_prob=None, concat=True):\n",
    "    \"\"\"\n",
    "    Implements (Bi-)LSTM, (Bi-)GRU and (Bi-)RNN\n",
    "    Args:\n",
    "        rnn_type: the type of rnn\n",
    "        inputs: padded inputs into rnn\n",
    "        length: the valid length of the inputs\n",
    "        hidden_size: the size of hidden units\n",
    "        layer_num: multiple rnn layer are stacked if layer_num > 1\n",
    "        dropout_keep_prob:\n",
    "        concat: When the rnn is bidirectional, the forward outputs and backward outputs are\n",
    "                concatenated if this is True, else we add them.\n",
    "    Returns:\n",
    "        RNN outputs and final state\n",
    "    \"\"\"\n",
    "    if not rnn_type.startswith('bi'):\n",
    "        cell = get_cell(rnn_type, hidden_size, layer_num, dropout_keep_prob)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, sequence_length=length, dtype=tf.float32)\n",
    "        \n",
    "        if rnn_type.endswith('lstm'):\n",
    "            c = [state.c for state in states]\n",
    "            h = [state.h for state in states]\n",
    "            states = h\n",
    "    else:\n",
    "        cell_fw = get_cell(rnn_type, hidden_size, layer_num, dropout_keep_prob)\n",
    "        cell_bw = get_cell(rnn_type, hidden_size, layer_num, dropout_keep_prob)\n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_bw, cell_fw, inputs, sequence_length=length, dtype=tf.float32)\n",
    "        states_fw, states_bw = states\n",
    "        \n",
    "        if rnn_type.endswith('lstm'):\n",
    "            c_fw = [state_fw.c for state_fw in states_fw]\n",
    "            h_fw = [state_fw.h for state_fw in states_fw]\n",
    "            c_bw = [state_bw.c for state_bw in states_bw]\n",
    "            h_bw = [state_bw.h for state_bw in states_bw]\n",
    "            states_fw, states_bw = h_fw, h_bw\n",
    "            \n",
    "        if concat:\n",
    "            outputs = tf.concat(outputs, 2)\n",
    "            states = tf.concat([states_fw, states_bw], 1)\n",
    "        else:\n",
    "            outputs = outputs[0] + outputs[1]\n",
    "            states = states_fw + states_bw\n",
    "            \n",
    "    return outputs, states\n",
    "\n",
    "\n",
    "def get_cell(rnn_type, hidden_size, layer_num=1, dropout_keep_prob=None):\n",
    "    \"\"\"\n",
    "    Gets the RNN Cell\n",
    "    Args:\n",
    "        rnn_type: 'lstm', 'gru' or 'rnn'\n",
    "        hidden_size: The size of hidden units\n",
    "        layer_num: MultiRNNCell are used if layer_num > 1\n",
    "        dropout_keep_prob: dropout in RNN\n",
    "    Returns:\n",
    "        An RNN Cell\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for i in range(layer_num):\n",
    "        if rnn_type.endswith('lstm'):\n",
    "            cell = tc.rnn.LSTMCell(num_units=hidden_size, initializer=tf.keras.initializers.glorot_normal(), state_is_tuple=True)\n",
    "        elif rnn_type.endswith('gru'):\n",
    "            cell = tc.rnn.GRUCell(num_units=hidden_size)\n",
    "        elif rnn_type.endswith('rnn'):\n",
    "            cell = tc.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "        else:\n",
    "            raise NotImplementedError('Unsuported rnn type: {}'.format(rnn_type))\n",
    "        if dropout_keep_prob is not None:\n",
    "            cell = tc.rnn.DropoutWrapper(cell,\n",
    "                                         input_keep_prob=dropout_keep_prob,\n",
    "                                         output_keep_prob=dropout_keep_prob)\n",
    "        cells.append(cell)\n",
    "    cells = tc.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    return cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "# from layers.basic_rnn import rnn\n",
    "# from layers.attention_layer import AttentionLayer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import Union\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "class TextbiRNN_att(object):\n",
    "    \"\"\"\n",
    "    Implements the main Self-attention TextClassification \n",
    "    \"\"\"\n",
    "    def __init__(self, args, vocab):\n",
    "\n",
    "        # logging\n",
    "        self.logger = logging.getLogger(\"RC\")\n",
    "        self.log_every_n_batch = args.log_every_n_batch\n",
    "\n",
    "        # basic config\n",
    "        self.seq_length = args.seq_length\n",
    "        self.num_classes = args.num_classes\n",
    "        self.use_dropout = args.use_dropout\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.attention_size = args.attention_size\n",
    "\n",
    "        self.optim_type = args.optim\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.clip = args.clip\n",
    "        self.weight_decay = args.weight_decay\n",
    "        \n",
    "        # the vocab\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # extra config\n",
    "        self.init_value = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        self.initializer_layer = initializers.xavier_initializer()\n",
    "\n",
    "        # session info\n",
    "        sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        sess_config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "        self.sess = tf.Session(config=sess_config)\n",
    "\n",
    "        self._build_graph()\n",
    "\n",
    "        # save info\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # initialize the model\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Builds the computation graph with Tensorflow\n",
    "        \"\"\"\n",
    "        start_t = time.time()\n",
    "        self._setup_placeholders()\n",
    "        self._embed()\n",
    "        self._birnn()\n",
    "        self._attention()\n",
    "        self._classify()\n",
    "        self._compute_loss()\n",
    "        self._compute_acc()\n",
    "        self._create_train_op()\n",
    "        print('Time to build graph: {} s'.format(time.time() - start_t))\n",
    "        param_num = sum([np.prod(self.sess.run(tf.shape(v))) for v in self.all_params])\n",
    "        print('There are {} parameters in the model'.format(param_num))\n",
    "        \n",
    "\n",
    "\n",
    "    def _setup_placeholders(self):\n",
    "        \"\"\"\n",
    "        Placeholders\n",
    "        \"\"\"\n",
    "        # 输入和标签\n",
    "        self.inputs_x = tf.placeholder(tf.int32, [None, self.seq_length],\n",
    "                                       name='inputs_x')\n",
    "        self.labels_y = tf.placeholder(tf.int32, [None, self.num_classes],\n",
    "                                       name='labels_y')\n",
    "        # 词向量dropout保留的神经元比例\n",
    "        self.emb_keep_prob = tf.placeholder(tf.float32, name='emb_keep_prob')\n",
    "        # RNN的dropout保留的神经元比例\n",
    "        self.rnn_keep_prob = tf.placeholder(tf.float32, name='rnn_keep_prob')\n",
    "        # 全连接dropout保留的神经元比例\n",
    "        self.fc_keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "        # 输入的实际长度\n",
    "        self.inputs_length = tf.placeholder(tf.int32, [None], name='input_length')\n",
    "\n",
    "    # 词向量层\n",
    "    def _embed(self):\n",
    "        \"\"\"\n",
    "        The embedding layer\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('embeddings'):\n",
    "            self.word_embeddings = tf.get_variable(name='word_embeddings',\n",
    "                                                   shape=[self.vocab.size, self.vocab.embed_dim],\n",
    "                                                   initializer=tf.constant_initializer(self.vocab.embeddings),\n",
    "                                                   trainable=True)\n",
    "\n",
    "            \n",
    "            self.x_emb = tf.nn.embedding_lookup(self.word_embeddings, self.inputs_x)\n",
    "            \n",
    "        if self.use_dropout:\n",
    "            self.x_emb = tf.nn.dropout(self.x_emb, self.emb_keep_prob)\n",
    "\n",
    "        self.input_emb = self.x_emb          \n",
    "\n",
    "    def _birnn(self):\n",
    "        \"\"\"\n",
    "        The BiLSTM layer\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('birnn'):\n",
    "            self.x_encode, _ = rnn('bi-lstm', self.input_emb, self.inputs_length, self.hidden_size, \n",
    "                                    dropout_keep_prob=self.rnn_keep_prob if self.use_dropout else None, \n",
    "                                    concat=False)\n",
    "\n",
    "    def _attention(self):\n",
    "        \"\"\"\n",
    "        The self-attention layer\n",
    "        \"\"\"\n",
    "        att_layer = AttentionLayer(self.hidden_size, self.attention_size)\n",
    "        self.att_x_encode = att_layer.compute_attention(self.x_encode)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            self.att_x_encode = tf.nn.dropout(self.att_x_encode, self.fc_keep_prob)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    def _classify(self):\n",
    "        \"\"\"\n",
    "        The classify layer\n",
    "        \"\"\"\n",
    "        # 全连接层，后面接dropout以及relu激活\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # 分类输出层\n",
    "            W_output = tf.get_variable(\"W_output\", shape=[self.hidden_size, self.num_classes],\n",
    "                                       initializer=self.init_value)\n",
    "            b_output = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b_output\")\n",
    "            \n",
    "            self.logits = tf.nn.xw_plus_b(self.att_x_encode, W_output, b_output, name=\"fc2\")\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits, name='softmax'), 1, name=\"model_pred\")\n",
    "\n",
    "\n",
    "    def _compute_loss(self):\n",
    "        \"\"\"\n",
    "        The loss function\n",
    "        \"\"\"\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.labels_y)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        self.all_params = tf.trainable_variables()\n",
    "        \n",
    "        \n",
    "        if self.weight_decay > 0:\n",
    "            self.all_params = tf.trainable_variables()\n",
    "            with tf.variable_scope('l2_loss'):\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in self.all_params])\n",
    "            self.loss += self.weight_decay * l2_loss\n",
    "        \n",
    "        if self.clip > 0:\n",
    "            self.globle_step = tf.Variable(0,name=\"globle_step\",trainable=False)\n",
    "            self.tvars = tf.trainable_variables()\n",
    "            self.grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.tvars), self.clip)\n",
    "            \n",
    "    def _compute_acc(self):\n",
    "        \"\"\"\n",
    "        The acc\n",
    "        \"\"\"\n",
    "        correct_pred = tf.equal(tf.argmax(self.labels_y , 1), self.y_pred_cls)\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "    def _create_train_op(self):\n",
    "        \"\"\"\n",
    "        Selects the training algorithm and creates a train operation with it\n",
    "        \"\"\"\n",
    "        if self.optim_type == 'adagrad':\n",
    "            self.optimizer = tf.train.AdagradOptimizer(self.learning_rate)\n",
    "        elif self.optim_type == 'adam':\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        elif self.optim_type == 'rprop':\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        elif self.optim_type == 'sgd':\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        else:\n",
    "            raise NotImplementedError('Unsupported optimizer: {}'.format(self.optim_type))\n",
    "        \n",
    "        if self.clip>0:\n",
    "            self.train_op = self.optimizer.apply_gradients(zip(self.grads, self.tvars))\n",
    "        else:\n",
    "            self.train_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "    def _train_epoch(self, train_batches, emb_keep_prob, rnn_keep_prob, fc_keep_prob):\n",
    "        \"\"\"\n",
    "        Trains the model for a single epoch\n",
    "        Args:\n",
    "            train_batches: iterable batch data for training\n",
    "            emb_keep_prob: float value indicating dropout keep probability of embedding layer\n",
    "            rnn_keep_prob: float value indicating dropout keep probability of rnn layer\n",
    "            fc_keep_prob: float value indicating dropout keep probability of fully-connection layer\n",
    "        \"\"\"\n",
    "        total_num, total_loss, total_acc = 0, 0, 0\n",
    "        log_every_n_batch, n_batch_loss, n_batch_acc = self.log_every_n_batch, 0, 0\n",
    "        for bitx, batch in enumerate(train_batches, 1):\n",
    "            feed_dict = {self.inputs_x: batch['contents'],\n",
    "                         self.labels_y: batch['labels'],\n",
    "                         self.emb_keep_prob: emb_keep_prob,\n",
    "                         self.rnn_keep_prob: rnn_keep_prob,\n",
    "                         self.fc_keep_prob: fc_keep_prob,\n",
    "                         self.inputs_length: batch['contents_length']}\n",
    "            _, loss, acc = self.sess.run([self.train_op, self.loss, self.acc], feed_dict)\n",
    "            total_loss += loss * len(batch['contents'])\n",
    "            total_acc += acc * len(batch['contents'])\n",
    "            total_num += len(batch['contents'])\n",
    "            n_batch_loss += loss\n",
    "            n_batch_acc += acc\n",
    "            if log_every_n_batch > 0 and bitx % log_every_n_batch == 0:\n",
    "                print('Average loss and acc from batch {} to {} is {:>6.2} and {:>7.2%}'\n",
    "                      .format(bitx - log_every_n_batch + 1, \n",
    "                              bitx, \n",
    "                              n_batch_loss / log_every_n_batch, \n",
    "                              n_batch_acc / log_every_n_batch))\n",
    "                n_batch_loss = 0\n",
    "                n_batch_acc = 0\n",
    "        return 1.0 * total_loss / total_num, 1.0 * total_acc / total_num\n",
    "\n",
    "    def train(self, data, epochs, batch_size, save_dir, save_prefix, \n",
    "              emb_keep_prob=1.0, rnn_keep_prob=1.0, fc_keep_prob=1.0,\n",
    "              evaluate=True):\n",
    "        \"\"\"\n",
    "        Train the model with data\n",
    "        Args:\n",
    "            data: the TCDataset class \n",
    "            epochs: number of training epochs\n",
    "            batch_size: the size of one mini-batch\n",
    "            save_dir: the directory to save the model\n",
    "            save_prefix: the prefix indicating the model type\n",
    "            emb_keep_prob: float value indicating dropout keep probability of embedding layer\n",
    "            rnn_keep_prob: float value indicating dropout keep probability of rnn layer\n",
    "            fc_keep_prob: float value indicating dropout keep probability of fully-connection layer\n",
    "            evaluate: whether to evaluate the model on dev data after each epoch\n",
    "        \"\"\"\n",
    "        pad_id = self.vocab.get_id(self.vocab.pad_token)\n",
    "        best_acc_val = 0\n",
    "        best_f1_val = 0\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print('\\nTraining the model for epoch {}'.format(epoch))\n",
    "            train_batches = data.gen_mini_batches('train', batch_size)\n",
    "            train_loss, train_acc = self._train_epoch(train_batches, emb_keep_prob, rnn_keep_prob, fc_keep_prob)\n",
    "            print('The {} Epoch average train loss and acc is {:>6.2} and {:>7.2%}'.format(epoch, train_loss, train_acc))\n",
    "\n",
    "            # 验证评估\n",
    "            if evaluate:\n",
    "                self.logger.info('Evaluating the model after epoch {}'.format(epoch))\n",
    "                if data.dev_contents is not None:\n",
    "                    eval_batches = data.gen_mini_batches('dev', batch_size)\n",
    "                    # 计算验证集的f1、loss和accuracy\n",
    "                    f1_val, loss_val, acc_val = self.evaluate(eval_batches, batch_size, data)\n",
    "                    print('Dev eval loss: {:>6.2}'.format(loss_val))\n",
    "                    print('Dev eval acc: {:>7.2%}'.format(acc_val))\n",
    "                    print('Dev eval f1: {:>7.2%}'.format(f1_val))\n",
    "\n",
    "                    if f1_val > best_f1_val:\n",
    "                        self.save(save_dir, save_prefix)\n",
    "                        best_f1_val = f1_val\n",
    "                    else:\n",
    "                        print('No promote')\n",
    "                        model.restore(save_dir, save_prefix)\n",
    "                        \n",
    "                else:\n",
    "                    self.logger.warning('No dev set is loaded for evaluation in the dataset!')\n",
    "\n",
    "            else:\n",
    "                self.save(save_dir, save_prefix + '_' + str(epoch))\n",
    "\n",
    "    def evaluate(self, eval_batches, batch_size, data, test=False):\n",
    "        \"\"\"\n",
    "        evaluate the model on dev data or test data\n",
    "        Args:\n",
    "            eval_batches: the eval data\n",
    "            data: the TCDataset class \n",
    "            test: whether to choose test mode\n",
    "        \"\"\"\n",
    "        if test:\n",
    "            data_len = data.test_size\n",
    "        else:\n",
    "            data_len = data.dev_size\n",
    "            \n",
    "        num_batch = int((data_len - 1) / batch_size) + 1\n",
    "        num_batch_list = list(range(num_batch))\n",
    "\n",
    "        y_test_cls = np.zeros(shape=data_len, dtype=np.int32)\n",
    "        y_pred_cls = np.zeros(shape=data_len, dtype=np.int32)  # 保存预测结果\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        \n",
    "        for i, batch in enumerate(eval_batches):  # 逐批次处理\n",
    "            start_id = num_batch_list[i] * batch_size\n",
    "            end_id = min((num_batch_list[i]  + 1) * batch_size, data_len)\n",
    "            feed_dict = {self.inputs_x: batch['contents'],\n",
    "                         self.labels_y: batch['labels'],\n",
    "                         self.emb_keep_prob: 1.0,\n",
    "                         self.rnn_keep_prob: 1.0,\n",
    "                         self.fc_keep_prob: 1.0,\n",
    "                         self.inputs_length: batch['contents_length']}\n",
    "            \n",
    "            y_pred_cls[start_id:end_id], loss, acc = self.sess.run([self.y_pred_cls, self.loss, self.acc],\n",
    "                                                                   feed_dict=feed_dict)\n",
    "            y_test_cls[start_id:end_id] = np.argmax(batch['labels'], 1)\n",
    "            \n",
    "            batch_len = len(batch['contents'])\n",
    "            total_loss += loss * batch_len\n",
    "            total_acc += acc * batch_len\n",
    "            \n",
    "        # 评估\n",
    "        f1 = f1_score(y_test_cls, y_pred_cls, average=\"macro\")\n",
    "        \n",
    "        if test:\n",
    "            self.cr_report = classification_report(y_test_cls, y_pred_cls, target_names=sorted(set(data.test_labels)))\n",
    "            self.cm_report = confusion_matrix(y_test_cls, y_pred_cls)\n",
    "            self.y_pred_cls = y_pred_cls\n",
    "            \n",
    "            print('Test eval loss: {:>6.2}'.format(total_loss / data_len))\n",
    "            print('Test eval acc: {:>7.2%}'.format(total_acc / data_len))\n",
    "            print('Test eval f1: {:>7.2%}'.format(f1))                    \n",
    "        else:\n",
    "             return f1, total_loss / data_len, total_acc / data_len\n",
    "\n",
    "\n",
    "    def save(self, model_dir, model_prefix):\n",
    "        \"\"\"\n",
    "        Saves the model into model_dir with model_prefix as the model indicator\n",
    "        Args:\n",
    "            model_dir: the save path\n",
    "            model_prefix: the prefix indicating the model type\n",
    "        \"\"\"\n",
    "        self.saver.save(self.sess, os.path.join(model_dir, model_prefix))\n",
    "        self.logger.info('Model saved in {}, with prefix {}.'.format(model_dir, model_prefix))\n",
    "\n",
    "    def restore(self, model_dir, model_prefix):\n",
    "        \"\"\"\n",
    "        Restores the model into model_dir from model_prefix as the model indicator\n",
    "        Args:\n",
    "            model_dir: the load path\n",
    "            model_prefix: the prefix indicating the model type\n",
    "        \"\"\"\n",
    "        self.saver.restore(self.sess, os.path.join(model_dir, model_prefix))\n",
    "        self.logger.info('Model restored from {}, with prefix {}'.format(model_dir, model_prefix))\n",
    "        \n",
    "        \n",
    "    def save_report(self, data, result_dir, save_suffix: Union[datetime, str, None] = None):\n",
    "        \"\"\"\n",
    "        Saves the model into model_dir with model_prefix as the model indicator\n",
    "        Args:\n",
    "            data: the TCDataset class \n",
    "            result_dir: the save path\n",
    "            save_suffix: the suffix \n",
    "        \"\"\"\n",
    "        if save_suffix is None:\n",
    "            save_suffix = datetime.now().strftime('%Y-%m-%d-%H-%M') \n",
    "            \n",
    "        report_data = [] \n",
    "        lines = self.cr_report.split('\\n') \n",
    "        for line in lines[2:-5]:\n",
    "            row = {}\n",
    "            row_data = line.split()\n",
    "            row['class'] = row_data[0]\n",
    "            row['precision'] = float(row_data[1])\n",
    "            row['recall'] = float(row_data[2])\n",
    "            row['f1_score'] = float(row_data[3])\n",
    "            row['support'] = float(row_data[4])\n",
    "            report_data.append(row)\n",
    "        df = pd.DataFrame.from_dict(report_data)\n",
    "        cr_filename = f'{result_dir}/classification_report_{save_suffix}.csv'\n",
    "        df.to_csv(cr_filename, index = False)\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(self.cm_report)\n",
    "        df.columns = sorted(set(data.test_labels))\n",
    "        \n",
    "        df = df.rename(index=data.id2cat)\n",
    "        cm_filename = f'{result_dir}/confusion_matrix_{save_suffix}.csv'\n",
    "        df.to_csv(cm_filename)\n",
    "        \n",
    "        # 预测与实际值统计\n",
    "        df = pd.DataFrame(self.y_pred_cls)\n",
    "        df.columns = ['predict']\n",
    "\n",
    "        df['predict'] = df['predict'].apply(lambda x: data.id2cat[x] )\n",
    "        df['label'] = data.test_labels\n",
    "        compare_filename = f'{result_dir}/predictAndlabel_{save_suffix}.csv'\n",
    "        df.to_csv(compare_filename)\n",
    "        \n",
    "    def write_rc_results(self, data, result_dir, save_suffix: Union[datetime, str, None] = None):\n",
    "        df = pd.DataFrame(self.y_pred_cls)\n",
    "        df.columns = ['predict']\n",
    "        df['predict'] = df['predict'].apply(lambda x: data.id2cat[x] )\n",
    "        \n",
    "        if save_suffix is None:\n",
    "            save_suffix = datetime.now().strftime('%Y-%m-%d-%H-%M')            \n",
    "        \n",
    "        results_file = f'{result_dir}/result_{save_suffix}.txt'\n",
    "        \n",
    "        start_no = 8001\n",
    "        with open(results_file, 'w') as f:\n",
    "            for idx, rel in enumerate(df['predict'].tolist()):\n",
    "                f.write('%d\\t%s\\n' % (start_no+idx, rel))\n",
    "            \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、设置必要的参数并创建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    \"\"\"模型配置类\"\"\"\n",
    "    \n",
    "    # 日志配置\n",
    "    log_every_n_batch = 100  # 每多少个batch打印一次日志    \n",
    "    \n",
    "    # 基础配置\n",
    "    # 1. 整体参数\n",
    "    seq_length = dataSet.max_content_len  # 序列长度\n",
    "    num_classes = dataSet.num_class  # 标签数\n",
    "    use_dropout = True  # 是否开启dropout\n",
    "    # 2. 词向量\n",
    "    dropout_emb_keep_prob = 0.5  # 词向量层dropout保留比例\n",
    "    # 3. RNN参数\n",
    "    hidden_size = 300  # 隐藏层神经元\n",
    "    attention_size = dataSet.max_content_len  # 注意力机制的维度\n",
    "    dropout_rnn_keep_prob = 0.7  # rnn层dropout保留比例\n",
    "    # 4. 全连接层\n",
    "    dropout_fc_keep_prob = 0.5  # 全连接层dropout保留比例\n",
    "    \n",
    "    # 优化算法配置\n",
    "    optim = 'adam'  # 所选的优化算法\n",
    "    learning_rate =0.001 # 学习率\n",
    "    clip = 1  # 梯度裁剪的限制，当值为0时，不开启梯度裁剪\n",
    "    weight_decay =  0.00005  # L2正则，当值为0时，不开启正则\n",
    "    \n",
    "    \n",
    "    # 训练配置\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 20  # 总迭代轮次\n",
    "    \n",
    "    # 保存配置\n",
    "    save_dir = './checkpoints/'\n",
    "    save_prefix = 'RC_V1'\n",
    "    report_dir = './data'\n",
    "    results_dir = './data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 消除当前已经建立的静态图\n",
    "import tensorflow.contrib.keras as kr\n",
    "kr.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextbiRNN_att(args, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.restore(save_dir, save_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(dataSet, \n",
    "            args.num_epochs, \n",
    "            args.batch_size, \n",
    "            args.save_dir,\n",
    "            args.save_prefix, \n",
    "            args.dropout_emb_keep_prob, \n",
    "            args.dropout_rnn_keep_prob, \n",
    "            args.dropout_fc_keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、测试和结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = dataSet.gen_mini_batches('test', args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_batches, args.batch_size, dataSet, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_report(dataSet, args.report_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write_rc_results(dataSet, args.results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
